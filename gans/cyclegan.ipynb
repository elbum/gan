{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN [with horse2zebra dataset]\n",
    "\n",
    "* `Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks`, [arXiv:1703.10593](https://arxiv.org/abs/1703.10593)\n",
    "  * Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros\n",
    "\n",
    "* This code is available to tensorflow version 2.0\n",
    "* Implemented by [`tf.keras.layers`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers) [`tf.losses`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:17:25.962441Z",
     "start_time": "2019-03-10T03:17:23.026084Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import PIL\n",
    "import imageio\n",
    "from IPython import display\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.abspath('.')))\n",
    "from utils.image_utils import *\n",
    "from utils.ops import *\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:17:25.980842Z",
     "start_time": "2019-03-10T03:17:25.975968Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training Flags (hyperparameter configuration)\n",
    "model_name = 'cyclegan'\n",
    "train_dir = os.path.join('train', model_name, 'exp1')\n",
    "\n",
    "constant_lr_epochs = 100\n",
    "decay_lr_epochs = 100\n",
    "max_epochs = constant_lr_epochs + decay_lr_epochs\n",
    "save_model_epochs = 20\n",
    "print_steps = 50\n",
    "save_images_epochs = 5\n",
    "batch_size = 1\n",
    "learning_rate_D = 2e-4\n",
    "learning_rate_G = 2e-4\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "IMG_SIZE = 256\n",
    "assert IMG_SIZE in [128, 256]\n",
    "LAMBDA = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "You can download this dataset and similar datasets from [here](https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/). \n",
    "This script source is borrowed from [original CycleGAN github repo.](https://github.com/junyanz/CycleGAN/blob/master/datasets/download_dataset.sh)\n",
    "\n",
    "\n",
    "As mentioned in the [paper](https://arxiv.org/abs/1703.10593) we apply random jittering and mirroring to the training dataset.\n",
    "* In random jittering, the image is resized to 286 x 286 and then randomly cropped to 256 x 256\n",
    "* In random mirroring, the image is randomly flipped horizontally i.e left to right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:17:25.998030Z",
     "start_time": "2019-03-10T03:17:25.987489Z"
    }
   },
   "outputs": [],
   "source": [
    "DATASETS = [\"ae_photos\",\n",
    "            \"apple2orange\",\n",
    "            \"summer2winter_yosemite\",\n",
    "            \"horse2zebra\",\n",
    "            \"monet2photo\",\n",
    "            \"cezanne2photo\",\n",
    "            \"ukiyoe2photo\",\n",
    "            \"vangogh2photo\",\n",
    "            \"maps\",\n",
    "            \"cityscapes\",\n",
    "            \"facades\",\n",
    "            \"iphone2dslr_flower\",\n",
    "            \"ae_photos\"]\n",
    "\n",
    "dataset_name = \"horse2zebra\"\n",
    "#dataset_name = \"cityscapes\"\n",
    "\n",
    "url = 'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/' + dataset_name + '.zip'\n",
    "datasets_path = '../datasets'\n",
    "if not os.path.isdir(datasets_path):\n",
    "  os.makedirs(datasets_path)\n",
    "zipfile_path = os.path.join(datasets_path, dataset_name + '.zip')\n",
    "\n",
    "# Download dataset\n",
    "if not os.path.isfile(zipfile_path):\n",
    "  urllib.request.urlretrieve(url=url, filename=zipfile_path)\n",
    "  print('download done')\n",
    "else:\n",
    "  print('zipfile already exists')\n",
    "\n",
    "# Extract zipfile\n",
    "PATH = os.path.join(datasets_path, dataset_name)\n",
    "if not os.path.isdir(PATH):\n",
    "  zip_ref = zipfile.ZipFile(zipfile_path, 'r')\n",
    "  zip_ref.extractall(datasets_path)\n",
    "  print('zipfile extract done')\n",
    "else:\n",
    "  print('zipfile already extracted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up dataset with `tf.data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(image_file):\n",
    "  image = tf.io.read_file(image_file)\n",
    "  image = tf.image.decode_jpeg(image, channels=3) # fix the output channels for intentionally\n",
    "\n",
    "  input_image = tf.cast(image, tf.float32)\n",
    "\n",
    "  return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageA = load(glob.glob(os.path.join(PATH, 'trainA/*.jpg'))[1])\n",
    "imageB = load(glob.glob(os.path.join(PATH, 'trainB/*.jpg'))[1])\n",
    "# casting to int for matplotlib to show the image\n",
    "plt.figure()\n",
    "plt.imshow(imageA/255.0)\n",
    "plt.figure()\n",
    "plt.imshow(imageB/255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(input_image, height, width):\n",
    "  input_image = tf.image.resize(input_image, [height, width],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "  \n",
    "  return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(input_image):\n",
    "  input_image = tf.image.random_crop(input_image, size=[IMG_SIZE, IMG_SIZE, 3])\n",
    "\n",
    "  return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the images to [-1, 1]\n",
    "def normalize(input_image):\n",
    "  input_image = tf.clip_by_value(input_image, 0.0, 255.0)\n",
    "  input_image = (input_image / 127.5) - 1\n",
    "\n",
    "  return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def random_jitter(input_image):\n",
    "  # resizing to 286 x 286 x 3\n",
    "  if IMG_SIZE == 256:\n",
    "    RESIZE = 286\n",
    "  else:\n",
    "    RESIZE = 145\n",
    "  input_image = resize(input_image, RESIZE, RESIZE)\n",
    "\n",
    "  # randomly cropping to 256 x 256 x 3\n",
    "  input_image = random_crop(input_image)\n",
    "\n",
    "  if tf.random.uniform(()) > 0.5:\n",
    "    # random mirroring\n",
    "    input_image = tf.image.flip_left_right(input_image)\n",
    "\n",
    "  return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you can see in the images below\n",
    "# that they are going through random jittering\n",
    "# Random jittering as described in the paper is to\n",
    "# 1. Resize an image to bigger height and width\n",
    "# 2. Randomnly crop to the original size\n",
    "# 3. Randomnly flip the image horizontally\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "for i in range(4):\n",
    "  rj_imageA = random_jitter(imageA)\n",
    "  plt.subplot(2, 2, i+1)\n",
    "  plt.imshow(rj_imageA/255.0)\n",
    "  plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_train(image_file):\n",
    "  input_image = load(image_file)\n",
    "  input_image = random_jitter(input_image)\n",
    "  input_image = normalize(input_image)\n",
    "\n",
    "  return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_test(image_file):\n",
    "  input_image = load(image_file)\n",
    "  input_image = resize(input_image, IMG_SIZE, IMG_SIZE)\n",
    "  input_image = normalize(input_image)\n",
    "\n",
    "  return input_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input pipeline\n",
    "\n",
    "* Use tf.data to create batches, map(do preprocessing) and shuffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:17:26.139132Z",
     "start_time": "2019-03-10T03:17:26.025346Z"
    }
   },
   "outputs": [],
   "source": [
    "N_trainX = len(glob.glob(os.path.join(PATH, 'trainA/*.jpg')))\n",
    "trainX_dataset = tf.data.Dataset.list_files(os.path.join(PATH, 'trainA/*.jpg'))\n",
    "trainX_dataset = trainX_dataset.shuffle(N_trainX)\n",
    "trainX_dataset = trainX_dataset.map(load_image_train,\n",
    "                                    #num_parallel_calls=tf.data.experimental.AUTOTUNE # Error of out of memory\n",
    "                                    num_parallel_calls=16)\n",
    "trainX_dataset = trainX_dataset.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:17:26.216373Z",
     "start_time": "2019-03-10T03:17:26.141676Z"
    }
   },
   "outputs": [],
   "source": [
    "N_trainY = len(glob.glob(os.path.join(PATH, 'trainB/*.jpg')))\n",
    "trainY_dataset = tf.data.Dataset.list_files(os.path.join(PATH, 'trainB/*.jpg'))\n",
    "trainY_dataset = trainY_dataset.shuffle(N_trainY)\n",
    "trainY_dataset = trainY_dataset.map(load_image_train,\n",
    "                                    #num_parallel_calls=tf.data.experimental.AUTOTUNE # Error of out of memory\n",
    "                                    num_parallel_calls=16)\n",
    "trainY_dataset = trainY_dataset.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:17:26.253049Z",
     "start_time": "2019-03-10T03:17:26.218390Z"
    }
   },
   "outputs": [],
   "source": [
    "N_testX = len(glob.glob(os.path.join(PATH, 'testA/*.jpg')))\n",
    "testX_dataset = tf.data.Dataset.list_files(os.path.join(PATH, 'testA/*.jpg'))\n",
    "# shuffling so that for every epoch a different image is generated\n",
    "# to predict and display the progress of our model.\n",
    "testX_dataset = testX_dataset.shuffle(N_testX*3)\n",
    "testX_dataset = testX_dataset.map(load_image_test)\n",
    "testX_dataset = testX_dataset.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:17:26.288530Z",
     "start_time": "2019-03-10T03:17:26.255501Z"
    }
   },
   "outputs": [],
   "source": [
    "N_testY = len(glob.glob(os.path.join(PATH, 'testB/*.jpg')))\n",
    "testY_dataset = tf.data.Dataset.list_files(os.path.join(PATH, 'testB/*.jpg'))\n",
    "# shuffling so that for every epoch a different image is generated\n",
    "# to predict and display the progress of our model.\n",
    "testY_dataset = testY_dataset.shuffle(N_testY*3)\n",
    "testY_dataset = testY_dataset.map(load_image_test)\n",
    "testY_dataset = testY_dataset.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:17:26.295641Z",
     "start_time": "2019-03-10T03:17:26.290735Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"number of examples in trainA: {}\".format(N_trainX))\n",
    "print(\"number of examples in trainB: {}\".format(N_trainY))\n",
    "print(\"number of examples in testA: {}\".format(N_testX))\n",
    "print(\"number of examples in testB: {}\".format(N_testY))\n",
    "N = min(N_trainX, N_trainY)\n",
    "print(\"number of examples in one epoch: {}\".format(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the generator and discriminator models\n",
    "\n",
    "### Generator\n",
    "  * The architecture of generator is based on [Johnson's architecture](https://arxiv.org/abs/1603.08155).\n",
    "  * Conv block in the generator is (Conv -> Batchnorm -> ReLU)\n",
    "  * Res block in the generator is (Conv -> ReLU -> Conv -> add X)\n",
    "  * ConvTranspose block in the generator is (Transposed Conv -> Batchnorm -> ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstanceNormalization(layers.Layer):\n",
    "  \"\"\"InstanceNormalization for only 4-rank Tensor (image data)\n",
    "  \"\"\"\n",
    "  def __init__(self, epsilon=1e-5):\n",
    "    super(InstanceNormalization, self).__init__()\n",
    "    self.epsilon = epsilon\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    shape = tf.TensorShape(input_shape)\n",
    "    param_shape = shape[-1]\n",
    "    # Create a trainable weight variable for this layer.\n",
    "    self.gamma = self.add_weight(name='gamma',\n",
    "                                 shape=param_shape,\n",
    "                                 initializer='ones',\n",
    "                                 trainable=True)\n",
    "    self.beta = self.add_weight(name='beta',\n",
    "                                shape=param_shape,\n",
    "                                initializer='zeros',\n",
    "                                trainable=True)\n",
    "    # Make sure to call the `build` method at the end\n",
    "    super(InstanceNormalization, self).build(input_shape)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # Compute the axes along which to reduce the mean / variance\n",
    "    input_shape = inputs.get_shape()\n",
    "    reduction_axes = [1, 2] # only shape index\n",
    "    mean, variance = tf.nn.moments(inputs, reduction_axes, keepdims=True)\n",
    "    normalized = (inputs - mean) / tf.sqrt(variance + self.epsilon)\n",
    "    return self.gamma * normalized + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:17:26.311154Z",
     "start_time": "2019-03-10T03:17:26.299181Z"
    }
   },
   "outputs": [],
   "source": [
    "class Conv(tf.keras.Model):\n",
    "  def __init__(self, filters, size, strides=1, padding='same', activation='relu',\n",
    "               apply_norm='instance', norm_momentum=0.9, norm_epsilon=1e-5):\n",
    "    super(Conv, self).__init__()\n",
    "    assert apply_norm in ['batch', 'instance', 'none']\n",
    "    self.apply_norm = apply_norm\n",
    "    assert activation in ['relu', 'tanh', 'none']\n",
    "    self.activation = activation\n",
    "    \n",
    "    if self.apply_norm == 'none':\n",
    "      use_bias = True\n",
    "    else:\n",
    "      use_bias = False\n",
    "    \n",
    "    self.conv = layers.Conv2D(filters=filters,\n",
    "                              kernel_size=(size, size),\n",
    "                              strides=strides,\n",
    "                              padding=padding,\n",
    "                              kernel_initializer=tf.random_normal_initializer(0., 0.02),\n",
    "                              use_bias=use_bias)\n",
    "    \n",
    "    if self.apply_norm == 'instance':\n",
    "      self.instancenorm = InstanceNormalization()\n",
    "    elif self.apply_norm == 'batch':\n",
    "      self.batchnorm = layers.BatchNormalization(momentum=norm_momentum,\n",
    "                                                 epsilon=norm_epsilon)\n",
    "    else:\n",
    "      pass\n",
    "  \n",
    "  def call(self, x, training):\n",
    "    # convolution\n",
    "    x = self.conv(x)\n",
    "    \n",
    "    # normalization\n",
    "    if self.apply_norm == 'instance':\n",
    "      x = self.instancenorm(x)\n",
    "    elif self.apply_norm == 'batch':\n",
    "      x = self.batchnorm(x, training=training)\n",
    "    else:\n",
    "      pass\n",
    "    \n",
    "    # activation\n",
    "    if self.activation == 'relu':\n",
    "      x = tf.nn.relu(x)\n",
    "    elif self.activation == 'tanh':\n",
    "      x = tf.nn.tanh(x)\n",
    "    else:\n",
    "      pass\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:17:26.322805Z",
     "start_time": "2019-03-10T03:17:26.314298Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResBlock(tf.keras.Model):\n",
    "  def __init__(self, filters, size):\n",
    "    super(ResBlock, self).__init__()\n",
    "    self.conv1 = Conv(filters, size, padding='valid', activation='relu')\n",
    "    self.conv2 = Conv(filters, size, padding='valid', activation='none')\n",
    "  \n",
    "  def call(self, x, training):\n",
    "    xp1 = tf.pad(x, [[0, 0], [1, 1], [1, 1], [0, 0]], 'REFLECT')\n",
    "    conv = self.conv1(xp1, training)\n",
    "    conv = tf.pad(conv, [[0, 0], [1, 1], [1, 1], [0, 0]], 'REFLECT')\n",
    "    conv = self.conv2(conv, training)\n",
    "    x = x + conv\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:17:26.342360Z",
     "start_time": "2019-03-10T03:17:26.325917Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConvTranspose(tf.keras.Model):\n",
    "  def __init__(self, filters, size,\n",
    "               apply_norm='instance', norm_momentum=0.9, norm_epsilon=1e-5):\n",
    "    super(ConvTranspose, self).__init__()\n",
    "    assert apply_norm in ['batch', 'instance']\n",
    "    self.apply_norm = apply_norm\n",
    "    self.up_conv = layers.Conv2DTranspose(filters=filters,\n",
    "                                          kernel_size=(size, size),\n",
    "                                          strides=2,\n",
    "                                          padding='same',\n",
    "                                          kernel_initializer=tf.random_normal_initializer(0., 0.02),\n",
    "                                          use_bias=False)\n",
    "    \n",
    "    if self.apply_norm == 'instance':\n",
    "      self.instancenorm = InstanceNormalization()\n",
    "    elif self.apply_norm == 'batch':\n",
    "      self.batchnorm = layers.BatchNormalization(momentum=norm_momentum,\n",
    "                                                 epsilon=norm_epsilon)\n",
    "    else:\n",
    "      pass\n",
    "\n",
    "  def call(self, x, training):\n",
    "    x = self.up_conv(x)\n",
    "    if self.apply_norm == 'instance':\n",
    "      x = self.instancenorm(x)\n",
    "    else:\n",
    "      x = self.batchnorm(x, training=training)\n",
    "    x = tf.nn.relu(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:17:26.360464Z",
     "start_time": "2019-03-10T03:17:26.345520Z"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "  def __init__(self, inputs_shape=256):\n",
    "    super(Generator, self).__init__()\n",
    "    assert inputs_shape in [128, 256]\n",
    "    self.inputs_shape = inputs_shape\n",
    "    self.conv = Conv(32, 7, padding='valid') # c7s1-32\n",
    "    self.down1 = Conv(64, 3, 2)  # d64\n",
    "    self.down2 = Conv(128, 3, 2) # d128\n",
    "    \n",
    "    self.res1 = ResBlock(128, 3) # R128\n",
    "    self.res2 = ResBlock(128, 3) # R128\n",
    "    self.res3 = ResBlock(128, 3) # R128\n",
    "    self.res4 = ResBlock(128, 3) # R128\n",
    "    self.res5 = ResBlock(128, 3) # R128\n",
    "    \n",
    "    if self.inputs_shape == 256:\n",
    "      self.res6 = ResBlock(128, 3) # R128\n",
    "      self.res7 = ResBlock(128, 3) # R128\n",
    "      self.res8 = ResBlock(128, 3) # R128\n",
    "      self.res9 = ResBlock(128, 3) # R128\n",
    "\n",
    "    self.up1 = ConvTranspose(64, 3) # u64\n",
    "    self.up2 = ConvTranspose(32, 3) # u32\n",
    "    self.last = Conv(3, 7, padding='valid', activation='tanh') # c7s1-3\n",
    "  \n",
    "  def call(self, x, training):\n",
    "    # x shape == (bs, 256, 256, 3)\n",
    "    xp1 = tf.pad(x, [[0, 0], [3, 3], [3, 3], [0, 0]], 'REFLECT') # xp1 shape: (bs, 262, 262, 3)\n",
    "    x1 = self.conv(xp1, training=training)    # x1 shape: (bs, 256, 256, 32)\n",
    "    x2 = self.down1(x1, training=training)    # x2 shape: (bs, 128, 128, 64)\n",
    "    x3 = self.down2(x2, training=training)    # x3 shape: (bs, 64, 64, 128)\n",
    "    \n",
    "    x4 = self.res1(x3, training=training)     # x4 shape: (bs, 64, 64, 128)\n",
    "    x5 = self.res2(x4, training=training)     # x5 shape: (bs, 64, 64, 128)\n",
    "    x6 = self.res3(x5, training=training)     # x6 shape: (bs, 64, 64, 128)\n",
    "    x7 = self.res4(x6, training=training)     # x7 shape: (bs, 64, 64, 128)\n",
    "    x8 = self.res5(x7, training=training)     # x8 shape: (bs, 64, 64, 128)\n",
    "    \n",
    "    if self.inputs_shape == 256:\n",
    "      x9 = self.res6(x8, training=training)   # x9 shape: (bs, 64, 64, 128)\n",
    "      x10 = self.res7(x9, training=training)  # x10 shape: (bs, 64, 64, 128)\n",
    "      x11 = self.res8(x10, training=training) # x11 shape: (bs, 64, 64, 128)\n",
    "      x12 = self.res9(x11, training=training) # x12 shape: (bs, 64, 64, 128)\n",
    "    else:\n",
    "      x12 = x8\n",
    "\n",
    "    x13 = self.up1(x12, training=training)    # x13 shape: (bs, 128, 128, 64)\n",
    "    x14 = self.up2(x13, training=training)    # x14 shape: (bs, 256, 256, 32)\n",
    "    xp2 = tf.pad(x14, [[0, 0], [3, 3], [3, 3], [0, 0]], 'REFLECT') # xp2 shape: (bs, 262, 262, 3)\n",
    "\n",
    "    generated_images = self.last(xp2, training=training) # generated_images shape: (bs, 256, 256, 3)\n",
    "\n",
    "    return generated_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two generators\n",
    "generator_X2Y = Generator(inputs_shape=IMG_SIZE) # This generator_X2Y corresponds to function G: X -> Y in paper's notation\n",
    "generator_Y2X = Generator(inputs_shape=IMG_SIZE) # This generator_Y2X corresponds to function F: Y -> X in paper's notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for Generator()\n",
    "fake_imageB = generator_X2Y(imageA[tf.newaxis, ...], training=False)\n",
    "fake_imageA = generator_Y2X(imageB[tf.newaxis, ...], training=False)\n",
    "plt.imshow(fake_imageB[0, ...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "  * The Discriminator is a PatchGAN.\n",
    "  * Each block in the discriminator is (Conv -> BatchNorm -> Leaky ReLU)\n",
    "  * The shape of the output after the last layer is (batch_size, 30, 30, 1)\n",
    "  * Each 30x30 patch of the output classifies a 70x70 portion of the input image (such an architecture is called a PatchGAN).\n",
    "  * Shape of the input travelling through the generator and the discriminator is in the comments in the code.\n",
    "\n",
    "To learn more about the architecture and the hyperparameters you can refer the [paper](https://arxiv.org/abs/1703.10593)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:17:26.375922Z",
     "start_time": "2019-03-10T03:17:26.363817Z"
    }
   },
   "outputs": [],
   "source": [
    "class DiscDownsample(tf.keras.Model):\n",
    "  def __init__(self, filters, size, strides=2,\n",
    "               apply_norm='instance', norm_momentum=0.9, norm_epsilon=1e-5,\n",
    "               apply_dropout=True):\n",
    "    super(DiscDownsample, self).__init__()\n",
    "    assert apply_norm in ['batch', 'instance', 'none']\n",
    "    self.apply_norm = apply_norm\n",
    "\n",
    "    if self.apply_norm == 'none':\n",
    "      use_bias = True\n",
    "    else:\n",
    "      use_bias = False\n",
    "      \n",
    "    self.apply_dropout = apply_dropout\n",
    "\n",
    "    self.conv = layers.Conv2D(filters=filters,\n",
    "                              kernel_size=(size, size),\n",
    "                              strides=strides,\n",
    "                              padding='same',\n",
    "                              kernel_initializer=tf.random_normal_initializer(0., 0.02),\n",
    "                              use_bias=use_bias)\n",
    "    \n",
    "    if self.apply_norm == 'instance':\n",
    "      self.instancenorm = InstanceNormalization()\n",
    "    elif self.apply_norm == 'batch':\n",
    "      self.batchnorm = layers.BatchNormalization(momentum=norm_momentum,\n",
    "                                                 epsilon=norm_epsilon)\n",
    "    else:\n",
    "      pass\n",
    "    \n",
    "    if self.apply_dropout:\n",
    "      self.dropout = layers.Dropout(0.5)\n",
    "  \n",
    "  def call(self, x, training):\n",
    "    # convolution\n",
    "    x = self.conv(x)\n",
    "    \n",
    "    # normalization\n",
    "    if self.apply_norm == 'instance':\n",
    "      x = self.instancenorm(x)\n",
    "    elif self.apply_norm == 'batch':\n",
    "      x = self.batchnorm(x, training=training)\n",
    "    else:\n",
    "      pass\n",
    "    \n",
    "    # dropout and activation\n",
    "    if self.apply_dropout:\n",
    "      x = self.dropout(x, training=training)\n",
    "    x = tf.nn.leaky_relu(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:17:26.394331Z",
     "start_time": "2019-03-10T03:17:26.380196Z"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Discriminator, self).__init__()    \n",
    "    self.down1 = DiscDownsample(64, 4, apply_dropout=False)             # C64\n",
    "    self.down2 = DiscDownsample(128, 4)                                  # C128\n",
    "    self.down3 = DiscDownsample(256, 4)                                  # C256\n",
    "    self.down4 = DiscDownsample(512, 4, strides=1, apply_dropout=False) # C512\n",
    "    self.last = Conv(1, 4, 1, activation='none', apply_norm='none')      # last\n",
    "  \n",
    "  def call(self, x, training):\n",
    "    # x shape == (bs, 256, 256, 3)\n",
    "    x = self.down1(x, training=training) # (bs, 128, 128, 64)\n",
    "    x = self.down2(x, training=training) # (bs, 64, 64, 128)\n",
    "    x = self.down3(x, training=training) # (bs, 32, 32, 256)\n",
    "    x = self.down4(x, training=training) # (bs, 32, 32, 512)\n",
    "    x = self.last(x, training=training)  # (bs, 32, 32, 1)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two discriminators\n",
    "discriminator_X = Discriminator() # This discriminator_X corresponds to function D_X in paper's notation\n",
    "discriminator_Y = Discriminator() # This discriminator_Y corresponds to function D_Y in paper's notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for Discriminator()\n",
    "disc_out = discriminator_X(imageA[tf.newaxis,...], training=False)\n",
    "disc_out = discriminator_Y(imageB[tf.newaxis,...], training=False)\n",
    "plt.imshow(disc_out[0,...,-1], vmin=-20, vmax=20, cmap='RdBu_r')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_X2Y.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_Y.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss functions and the optimizer\n",
    "\n",
    "* **Discriminator loss**\n",
    "  * The discriminator loss function takes 2 inputs; real images, generated images\n",
    "  * real_loss is a sigmoid cross entropy loss of the real images and an array of ones(since these are the real images)\n",
    "  * generated_loss is a sigmoid cross entropy loss of the generated images and an array of zeros(since these are the fake images)\n",
    "  * Then the total_loss is the sum of real_loss and the generated_loss\n",
    "* **Generator loss**\n",
    "  * It is a sigmoid cross entropy loss of the generated images and an array of ones.\n",
    "  * The paper also includes L1 loss which is MAE (mean absolute error) between the generated image and the target image.\n",
    "  * This allows the generated image to become structurally similar to the target image.\n",
    "  * The formula to calculate the total generator loss = gan_loss + LAMBDA * l1_loss, where LAMBDA = 100. This value was decided by the authors of the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_object = tf.losses.BinaryCrossentropy(from_logits=True)\n",
    "mse_object = tf.losses.MeanSquaredError()\n",
    "mae_object = tf.losses.MeanAbsoluteError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:17:26.503635Z",
     "start_time": "2019-03-10T03:17:26.488583Z"
    }
   },
   "outputs": [],
   "source": [
    "def GANLoss(logits, is_real=True, use_lsgan=True):\n",
    "  \"\"\"Computes standard GAN loss between `logits` and `labels`.\n",
    "\n",
    "  Args:\n",
    "    logits (`1-rank Tensor`): logits.\n",
    "    is_real (`bool`): True means `1` labeling, False means `0` labeling.\n",
    "    use_lsgan (`bool`): True means LSGAN loss, False means standard GAN loss\n",
    "\n",
    "  Returns:\n",
    "    loss (`0-randk Tensor): the standard GAN loss value. (binary_cross_entropy)\n",
    "                            or LSGAN loss value.\n",
    "  \"\"\"\n",
    "  if is_real:\n",
    "    labels = tf.ones_like(logits)\n",
    "  else:\n",
    "    labels = tf.zeros_like(logits)\n",
    "    \n",
    "  if use_lsgan:\n",
    "    loss = mse_object(labels, tf.nn.sigmoid(logits))\n",
    "  else:\n",
    "    loss = bce_object(labels, logits)\n",
    "    \n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:17:26.525520Z",
     "start_time": "2019-03-10T03:17:26.519670Z"
    }
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real_logits, fake_logits):\n",
    "  # losses of real with label \"1\"\n",
    "  real_loss = GANLoss(logits=real_logits, is_real=True)\n",
    "  # losses of fake with label \"0\"\n",
    "  fake_loss = GANLoss(logits=fake_logits, is_real=False)\n",
    "  \n",
    "  return real_loss + fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:17:26.536977Z",
     "start_time": "2019-03-10T03:17:26.529930Z"
    }
   },
   "outputs": [],
   "source": [
    "def cycle_consistency_loss(X, X2Y2X):\n",
    "  cycle_loss = mae_object(X, X2Y2X) # L1 loss\n",
    "  #cycle_loss = mse_object(X, X2Y2X) # L2 loss\n",
    "  return cycle_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:17:26.625649Z",
     "start_time": "2019-03-10T03:17:26.590116Z"
    }
   },
   "outputs": [],
   "source": [
    "def generator_loss(fake_logits, imagesX, generated_images_X2Y2X):\n",
    "  # losses of Generator with label \"1\" that used to fool the Discriminator\n",
    "  gan_loss = GANLoss(logits=fake_logits, is_real=True)\n",
    "  \n",
    "  # mean absolute error\n",
    "  cycle_loss = cycle_consistency_loss(imagesX, generated_images_X2Y2X)\n",
    "\n",
    "  return gan_loss + (LAMBDA * cycle_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:17:26.716423Z",
     "start_time": "2019-03-10T03:17:26.707166Z"
    }
   },
   "outputs": [],
   "source": [
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate_D, beta_1=0.5)\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate_G, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:17:26.733394Z",
     "start_time": "2019-03-10T03:17:26.722132Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = train_dir\n",
    "if not tf.io.gfile.exists(checkpoint_dir):\n",
    "  tf.io.gfile.makedirs(checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator_X2Y=generator_X2Y,\n",
    "                                 generator_Y2X=generator_Y2X,\n",
    "                                 discriminator_X=discriminator_X,\n",
    "                                 discriminator_Y=discriminator_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define `generate_and_print_or_save` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:17:26.772958Z",
     "start_time": "2019-03-10T03:17:26.763608Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_and_print_or_save_sample_images(inputs_X, inputs_Y,\n",
    "                                             is_save=False, epoch=None, checkpoint_dir=checkpoint_dir):\n",
    "  X2Y = generator_X2Y(inputs_X, training=False)\n",
    "  X2Y2X = generator_Y2X(X2Y, training=False)\n",
    "  print_or_save_sample_images_pix2pix(inputs_X, X2Y, X2Y2X,\n",
    "                                      model_name='cyclegan', name='X2Y2X',\n",
    "                                      is_save=is_save, epoch=epoch, checkpoint_dir=checkpoint_dir)\n",
    "\n",
    "  Y2X = generator_Y2X(inputs_Y, training=False)\n",
    "  Y2X2Y = generator_X2Y(Y2X, training=False)\n",
    "  print_or_save_sample_images_pix2pix(inputs_Y, Y2X, Y2X2Y,\n",
    "                                      model_name='cyclegan', name='Y2X2Y',\n",
    "                                      is_save=is_save, epoch=epoch, checkpoint_dir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:17:26.852862Z",
     "start_time": "2019-03-10T03:17:26.779111Z"
    }
   },
   "outputs": [],
   "source": [
    "# keeping the constant test input for generation (prediction) so\n",
    "# it will be easier to see the improvement of the pix2pix.\n",
    "for inputs_X, inputs_Y in zip(testX_dataset.take(1), testY_dataset.take(1)):\n",
    "  const_test_input_X = inputs_X\n",
    "  const_test_input_Y = inputs_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:17:30.508009Z",
     "start_time": "2019-03-10T03:17:26.856540Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check for test data X -> Y -> X\n",
    "# Check for test data Y -> X -> Y\n",
    "generate_and_print_or_save_sample_images(const_test_input_X, const_test_input_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:24:48.241117Z",
     "start_time": "2019-03-10T03:24:48.238689Z"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training one step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(imagesX, imagesY, global_step):\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    # Image generation from one domain to another domain\n",
    "    generated_images_X2Y = generator_X2Y(imagesX, training=True)  # G: X -> Y\n",
    "    generated_images_Y2X = generator_Y2X(imagesY, training=True)  # F: Y -> X\n",
    "    \n",
    "    # Image generation from one domain via another domain to original domain\n",
    "    generated_images_X2Y2X = generator_Y2X(generated_images_X2Y, training=True)  # F: Y -> X\n",
    "    generated_images_Y2X2Y = generator_X2Y(generated_images_Y2X, training=True)  # G: X -> Y\n",
    "\n",
    "    # Discriminate real images by Discriminator()\n",
    "    real_logits_X = discriminator_X(imagesX, training=True)  # D_X\n",
    "    real_logits_Y = discriminator_Y(imagesY, training=True)  # D_Y\n",
    "\n",
    "    # Discriminate generated (fake) images by Discriminator()\n",
    "    fake_logits_X2Y = discriminator_Y(generated_images_X2Y, training=True) # D_Y\n",
    "    fake_logits_Y2X = discriminator_X(generated_images_Y2X, training=True) # D_X\n",
    "\n",
    "    gen_X2Y_loss = generator_loss(fake_logits_X2Y, imagesX, generated_images_X2Y2X)\n",
    "    gen_Y2X_loss = generator_loss(fake_logits_Y2X, imagesY, generated_images_Y2X2Y)\n",
    "    disc_X_loss = discriminator_loss(real_logits_X, fake_logits_Y2X)\n",
    "    disc_Y_loss = discriminator_loss(real_logits_Y, fake_logits_X2Y)\n",
    "    \n",
    "    total_generator_loss = gen_X2Y_loss + gen_Y2X_loss\n",
    "    total_discriminator_loss = disc_X_loss + disc_Y_loss\n",
    "\n",
    "  discriminator_tvars = discriminator_X.trainable_variables + discriminator_Y.trainable_variables\n",
    "  generator_tvars = generator_X2Y.trainable_variables + generator_Y2X.trainable_variables\n",
    "  \n",
    "  gradients_of_discriminator = disc_tape.gradient(total_discriminator_loss, discriminator_tvars)\n",
    "  gradients_of_generator = gen_tape.gradient(total_generator_loss, generator_tvars)\n",
    "  \n",
    "  # Learning rate decay\n",
    "  num_steps_per_epoch = N // batch_size\n",
    "  if global_step > num_steps_per_epoch * constant_lr_epochs:\n",
    "    decay_step = num_steps_per_epoch * decay_lr_epochs\n",
    "    discriminator_optimizer.lr.assign_sub(learning_rate_D * 1. / decay_step) # tf.train.polynomial_decay (linear decay)\n",
    "    generator_optimizer.lr.assign_sub(learning_rate_G * 1. / decay_step) # tf.train.polynomial_decay (linear decay)\n",
    "\n",
    "  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator_tvars))\n",
    "  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator_tvars))\n",
    "\n",
    "  return gen_X2Y_loss, gen_Y2X_loss, disc_X_loss, disc_Y_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training until max_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:18:33.119012Z",
     "start_time": "2019-03-10T03:17:30.705494Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Start Training.')\n",
    "num_batches_per_epoch = N // batch_size\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "\n",
    "  # End of 'for' loop depends on shorter dataset\n",
    "  for step, (imagesX, imagesY) in enumerate(zip(trainX_dataset, trainY_dataset)):\n",
    "    start_time = time.time()\n",
    "\n",
    "    gen_X2Y_loss, gen_Y2X_loss, disc_X_loss, disc_Y_loss = train_step(imagesX, imagesY, global_step)\n",
    "    global_step.assign_add(1)\n",
    "\n",
    "    # print the result images every print_steps\n",
    "    if global_step.numpy() % print_steps == 0:\n",
    "      epochs = epoch + step / float(num_batches_per_epoch)\n",
    "      duration = time.time() - start_time\n",
    "      examples_per_sec = batch_size / float(duration)\n",
    "      display.clear_output(wait=True)\n",
    "      print(\"Epochs: {:.2f} lr: {:.3g}, {:.3g}, global_step: {:d} loss_D_X: {:.3g} loss_D_Y: {:.3g} loss_G_X2Y: {:.3g} loss_F_Y2X: {:.3g} ({:.2f} examples/sec; {:.3f} sec/batch)\".format(\n",
    "                epochs, generator_optimizer.lr.numpy(), discriminator_optimizer.lr.numpy(), global_step.numpy(), disc_X_loss, disc_Y_loss, gen_X2Y_loss, gen_Y2X_loss, examples_per_sec, duration))\n",
    "      # generate sample image from random test image\n",
    "      # the training=True is intentional here since\n",
    "      # we want the batch statistics while running the model\n",
    "      # on the test dataset. If we use training=False, we will get \n",
    "      # the accumulated statistics learned from the training dataset\n",
    "      # (which we don't want)\n",
    "      for test_inputs_X, test_inputs_Y in zip(testX_dataset.take(1), testY_dataset.take(1)):\n",
    "        generate_and_print_or_save_sample_images(test_inputs_X, test_inputs_Y)\n",
    "\n",
    "  # saving the result image files every save_images_epochs\n",
    "  if (epoch + 1) % save_images_epochs == 0:\n",
    "    display.clear_output(wait=True)\n",
    "    print(\"This images are saved at {} epoch\".format(epoch+1))\n",
    "    generate_and_print_or_save_sample_images(const_test_input_X, const_test_input_Y,\n",
    "                                             is_save=True, epoch=epoch+1, checkpoint_dir=checkpoint_dir)\n",
    "\n",
    "  # saving (checkpoint) the model every save_epochs\n",
    "  if (epoch + 1) % save_model_epochs == 0:\n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    \n",
    "print('Training Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:18:34.536883Z",
     "start_time": "2019-03-10T03:18:33.121400Z"
    }
   },
   "outputs": [],
   "source": [
    "# generating after the final epoch\n",
    "display.clear_output(wait=True)\n",
    "for test_inputs_X, test_inputs_Y in zip(testX_dataset.take(1), testY_dataset.take(1)):\n",
    "  generate_and_print_or_save_sample_images(test_inputs_X, test_inputs_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the latest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:18:35.376184Z",
     "start_time": "2019-03-10T03:18:34.540701Z"
    }
   },
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display an image using the epoch number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:18:35.861571Z",
     "start_time": "2019-03-10T03:18:35.396028Z"
    }
   },
   "outputs": [],
   "source": [
    "display_image(max_epochs, 'X2Y2X', checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:18:36.057470Z",
     "start_time": "2019-03-10T03:17:23.519Z"
    }
   },
   "outputs": [],
   "source": [
    "display_image(max_epochs, 'Y2X2Y', checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a GIF of all the saved images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:18:36.064979Z",
     "start_time": "2019-03-10T03:17:23.560Z"
    }
   },
   "outputs": [],
   "source": [
    "filename1 = model_name + '_' + dataset_name + '_' + 'X2Y2X' + '.gif'\n",
    "generate_gif(filename1, checkpoint_dir)\n",
    "filename2 = model_name + '_' + dataset_name + '_' + 'Y2X2Y' + '.gif'\n",
    "generate_gif(filename2, checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:18:36.068567Z",
     "start_time": "2019-03-10T03:17:23.563Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display.Image(filename=filename1 + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T03:18:36.072631Z",
     "start_time": "2019-03-10T03:17:23.566Z"
    }
   },
   "outputs": [],
   "source": [
    "display.Image(filename=filename2 + '.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
